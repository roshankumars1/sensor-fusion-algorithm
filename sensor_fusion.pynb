!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_drive_0005/2011_09_26_drive_0005_sync.zip
!wget https://s3.eu-central-1.amazonaws.com/avg-kitti/raw_data/2011_09_26_calib.zip
!jar xf 2011_09_26_drive_0005_sync.zip
!jar xf 2011_09_26_calib.zip
import os
from glob import glob
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

%matplotlib inline
plt.rcParams["figure.figsize"] = (20, 10)

!wget https://github.com/itberrios/CV_tracking/raw/main/kitti_tracker/kitti_utils.py
from kitti_utils import *

DATA_PATH = r'2011_09_26/2011_09_26_drive_0005_sync'
left_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_02/data/*.png')))
right_image_paths = sorted(glob(os.path.join(DATA_PATH, 'image_03/data/*.png')))
bin_paths = sorted(glob(os.path.join(DATA_PATH, 'velodyne_points/data/*.bin')))
oxts_paths = sorted(glob(os.path.join(DATA_PATH, r'oxts/data**/*.txt')))
print(f"Number of left images: {len(left_image_paths)}")
print(f"Number of right images: {len(right_image_paths)}")
print(f"Number of LiDAR point clouds: {len(bin_paths)}")
print(f"Number of GPS/IMU frames: {len(oxts_paths)}")

with open('2011_09_26/calib_cam_to_cam.txt','r') as f:
  calib = f.readlines()
P_rect2_cam2 = np.array([float(x) for x in calib[25].strip().split(' ')[1:]]).reshape((3,4))
R_ref0_rect2 = np.array([float(x) for x in calib[24].strip().split(' ')[1:]]).reshape((3, 3,))
R_ref0_rect2 = np.insert(R_ref0_rect2, 3, values=[0,0,0], axis=0)
R_ref0_rect2 = np.insert(R_ref0_rect2, 3, values=[0,0,0,1], axis=1)
R_2 = np.array([float(x) for x in calib[21].strip().split(' ')[1:]]).reshape((3,3))
t_2 = np.array([float(x) for x in calib[22].strip().split(' ')[1:]]).reshape((3,1))
T_ref0_ref2 = np.insert(np.hstack((R_2, t_2)), 3, values=[0,0,0,1], axis=0)
T_velo_ref0 = get_rigid_transformation(r'2011_09_26/calib_velo_to_cam.txt')
T_imu_velo = get_rigid_transformation(r'2011_09_26/calib_imu_to_velo.txt')
T_velo_cam2 = P_rect2_cam2 @ R_ref0_rect2 @ T_ref0_ref2 @ T_velo_ref0
T_cam2_velo = np.linalg.inv(np.insert(T_velo_cam2, 3, values=[0,0,0,1], axis=0))
T_imu_cam2 = T_velo_cam2 @ T_imu_velo
T_cam2_imu = np.linalg.inv(np.insert(T_imu_cam2, 3, values=[0,0,0,1], axis=0))
!git clone https://github.com/ultralytics/yolov5
!pip install -r yolov5/requirements.txt
import torch
model = torch.hub.load('ultralytics/yolov5', 'yolov5s')
model.conf = 0.25
model.iou = 0.25

def get_uvz_centers(image, velo_uvz, bboxes, draw=True):
    u, v, z = velo_uvz
    bboxes_out = np.zeros((bboxes.shape[0], bboxes.shape[1] + 3))
    bboxes_out[:, :bboxes.shape[1]] = bboxes
    for i, bbox in enumerate(bboxes):
        pt1 = torch.round(bbox[0:2]).to(torch.int).numpy()
        pt2 = torch.round(bbox[2:4]).to(torch.int).numpy()
        obj_x_center = (pt1[1] + pt2[1]) / 2
        obj_y_center = (pt1[0] + pt2[0]) / 2
        center_delta = np.abs(np.array((v, u))- np.array([[obj_x_center, obj_y_center]]).T)
        min_loc = np.argmin(np.linalg.norm(center_delta, axis=0))
        velo_depth = z[min_loc];
        uvz_location = np.array([u[min_loc], v[min_loc], velo_depth])
        bboxes_out[i, -3:] = uvz_location
        if draw:
            object_center = (np.round(obj_y_center).astype(int),np.round(obj_x_center).astype(int))
            cv2.putText(image,'{0:.2f} m'.format(velo_depth),object_center, cv2.FONT_HERSHEY_SIMPLEX, 0.5,(255, 0, 0), 2, cv2.LINE_AA)
    return bboxes_out

def get_detection_coordinates(image, bin_path, draw_boxes=True, draw_depth=True):
    detections = model(image)
    if draw_boxes:
        detections.show()
    bboxes = detections.xyxy[0].cpu()
    velo_uvz = project_velobin2uvz(bin_path,T_velo_cam2,image,remove_plane=True)
    bboxes = get_uvz_centers(image,velo_uvz,bboxes,draw=draw_depth)
    return bboxes, velo_uvz

!pip install pymap3d
import pymap3d as pm
def imu2geodetic(x, y, z, lat0, lon0, alt0, heading0):
    rng = np.sqrt(x**2 + y**2 + z**2)
    az = np.degrees(np.arctan2(y, x)) + np.degrees(heading0)
    el = np.degrees(np.arctan2(np.sqrt(x**2 + y**2), z)) + 90
    lla = pm.aer2geodetic(az, el, rng, lat0, lon0, alt0)
    lla = np.vstack((lla[0], lla[1], lla[2])).T
    return lla

index = 10
left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)
bin_path = bin_paths[index]
oxts_frame = get_oxts(oxts_paths[index])
bboxes, velo_uvz = get_detection_coordinates(left_image, bin_path)
uvz = bboxes[:, -3:]
imu_xyz = transform_uvz(uvz, T_cam2_imu)
lat0 = oxts_frame[0]
lon0 = oxts_frame[1]
alt0 = oxts_frame[2]
heading0 = oxts_frame[5]
lla = imu2geodetic(imu_xyz[:, 0], imu_xyz[:, 1], imu_xyz[:, 2], lat0, lon0, alt0, heading0)
velo_image = draw_velo_on_image(velo_uvz, np.zeros_like(left_image))
%matplotlib inline
plt.rcParams["figure.figsize"] = (20, 10)
stacked = np.vstack((left_image, velo_image))
plt.imshow(stacked);
left_image_2 = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)
velo_image_2 = draw_velo_on_image(velo_uvz, left_image_2)
plt.imshow(velo_image_2);
!pip install folium
import folium
drive_map = folium.Map(location=(lat0, lon0),zoom_start=18)
folium.CircleMarker(location=(lat0, lon0),radius=2,weight=5,color='red').add_to(drive_map);
for pos in lla:
    folium.CircleMarker(location=(pos[0], pos[1]),radius=2,weight=5,color='green').add_to(drive_map);
folium.CircleMarker(location=(lat0, lon0),radius=2,weight=5,color='red').add_to(drive_map);
canvas_height = stacked.shape[0]
canvas_width = 500
ego_center = (250, int(canvas_height*0.95))
ego_x1 = ego_center[0] - 5
ego_y1 = ego_center[1] - 10
ego_x2 = ego_center[0] + 5
ego_y2 = ego_center[1] + 10
def draw_scenario(canvas, imu_xyz, sf=12):
    cv2.rectangle(canvas, (ego_x1, ego_y1), (ego_x2, ego_y2), (0, 255, 0), -1);
    for val in imu_xyz:
        obj_center = (ego_center[0] - sf*int(np.round(val[1])),
                      ego_center[1] - sf*int(np.round(val[0])))
        obj_x1 = obj_center[0] - 5
        obj_y1 = obj_center[1] - 10
        obj_x2 = obj_center[0] + 5
        obj_y2 = obj_center[1] + 10
        cv2.rectangle(canvas, (obj_x1, obj_y1), (obj_x2, obj_y2), (255, 0, 0), -1);
    return canvas
canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
draw_scenario(canvas, imu_xyz, sf=12)
plt.imshow(canvas);
drive_map
images = []
for index in range(len(left_image_paths)):
    left_image = cv2.cvtColor(cv2.imread(left_image_paths[index]), cv2.COLOR_BGR2RGB)
    bin_path = bin_paths[index]
    oxts_frame = get_oxts(oxts_paths[index])
    bboxes, velo_uvz = get_detection_coordinates(left_image, bin_path)
    uvz = bboxes[:, -3:]
    imu_xyz = transform_uvz(uvz, T_cam2_imu)
    velo_image = draw_velo_on_image(velo_uvz, np.zeros_like(left_image))
    stacked = np.vstack((left_image, velo_image))
    canvas = np.zeros((canvas_height, canvas_width, 3), dtype=np.uint8)
    draw_scenario(canvas, imu_xyz, sf=12)
    frame = np.hstack((stacked,255*np.ones((canvas_height, 1, 3), dtype=np.uint8),canvas))
    images.append(frame)
h, w, _ = frame.shape
